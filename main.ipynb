{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32ef73ef-eed4-4f50-b314-04f9e6272fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download video from youtube, using pytube as API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "da327137-1518-402b-adfc-8dcedc55d52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pytube import YouTube\n",
    "import os\n",
    "\n",
    "def download_video(url, output_path='videos'):   ## downlod video under /video folder\n",
    "    \"\"\"\n",
    "    funtion to download videos\n",
    "    \"\"\"\n",
    "    try:\n",
    "\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        yt = YouTube(url)\n",
    "        video = yt.streams.filter(progressive=True, file_extension='mp4').order_by('resolution').desc().first()\n",
    "        \n",
    "        # download video\n",
    "        if video:\n",
    "            video.download(output_path=output_path)\n",
    "            print(f\"Downloaded video to {os.path.join(output_path, video.default_filename)}\")\n",
    "        else:\n",
    "            print(\"No mp4 video available for download.\")\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a951c36d-80da-4dab-a4b6-5581fd06d2c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded video to videos/Cyclist and vehicle Tracking - 1.mp4\n"
     ]
    }
   ],
   "source": [
    "## Call download_video function to download the video\n",
    "youtube_url = 'https://youtu.be/WeF4wpw7w9k'\n",
    "download_video(youtube_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d71ddb5b-97fe-4e19-9bd2-5bfc9a966b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded video to videos/Cyclist and vehicle tracking - 2.mp4\n"
     ]
    }
   ],
   "source": [
    "youtube_url = 'https://youtu.be/2NFwY15tRtA'\n",
    "download_video(youtube_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "052534e7-9143-4a1f-80cb-ebc5589076e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded video to videos/Drone Tracking Video.mp4\n"
     ]
    }
   ],
   "source": [
    "youtube_url = 'https://youtu.be/5dRramZVu2Q'\n",
    "download_video(youtube_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aeaf8d81-a3ee-4f8e-8fb4-a2c72ee8b459",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded video to videos/Dji Mavic air 2 drone using litchi app with follow me mode on a bike occluded by trees.mp4\n"
     ]
    }
   ],
   "source": [
    "youtube_url ='https://youtu.be/2hQx48U1L-Y'\n",
    "download_video(youtube_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f89e5df0-d653-4fc8-b901-b26df96e53da",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Video Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28745d73-2481-4465-8900-539c6a657071",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finetune the detection model using Visdrone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab09f77-074b-4700-a2d9-3d2ba91dfef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c131e12b-2578-4089-9000-1ac39a629329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting gdown\n",
      "  Downloading gdown-5.1.0-py3-none-any.whl.metadata (5.7 kB)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/vscode/.local/lib/python3.10/site-packages (from gdown) (4.12.3)\n",
      "Requirement already satisfied: filelock in /home/vscode/.local/lib/python3.10/site-packages (from gdown) (3.13.3)\n",
      "Requirement already satisfied: requests[socks] in /home/vscode/.local/lib/python3.10/site-packages (from gdown) (2.31.0)\n",
      "Requirement already satisfied: tqdm in /home/vscode/.local/lib/python3.10/site-packages (from gdown) (4.66.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/vscode/.local/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/vscode/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/vscode/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/vscode/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/vscode/.local/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.2.2)\n",
      "Collecting PySocks!=1.5.7,>=1.5.6 (from requests[socks]->gdown)\n",
      "  Downloading PySocks-1.7.1-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading gdown-5.1.0-py3-none-any.whl (17 kB)\n",
      "Downloading PySocks-1.7.1-py3-none-any.whl (16 kB)\n",
      "Installing collected packages: PySocks, gdown\n",
      "Successfully installed PySocks-1.7.1 gdown-5.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "21b30383-b33a-4d74-8164-bfe3b8a5b08f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From (original): https://drive.google.com/uc?id=1a2oHjcEcwXP8oUF95qiwrqzACb2YlUhn\n",
      "From (redirected): https://drive.google.com/uc?id=1a2oHjcEcwXP8oUF95qiwrqzACb2YlUhn&confirm=t&uuid=5ecafc02-e30d-4cec-bc47-6bc055396b04\n",
      "To: /workspaces/artificial_intelligence/Drone_follow_me/VisDrone2019-DET-train.zip\n",
      "100%|██████████████████████████████████████| 1.55G/1.55G [01:07<00:00, 22.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "!gdown 'https://drive.google.com/uc?id=1a2oHjcEcwXP8oUF95qiwrqzACb2YlUhn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05676aba-5c8e-4c6a-b746-a8fdce22c101",
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip VisDrone2019-DET-train.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ee6934-bcb0-4f21-88c8-ce7b8b533f7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load fintune dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c720346-643f-434e-9cfc-01f4ef1ac183",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4af1990a-3898-4dcb-82cb-26daa36409a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vscode/.local/lib/python3.10/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "\n",
    "def get_transform():\n",
    "    transform = Compose([\n",
    "        Resize((896, 896)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "734b0c4a-0c38-4f31-a27d-c8aa513c806f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision.transforms import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class VisDroneDataset(Dataset):\n",
    "    def __init__(self, img_dir, ann_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.ann_dir = ann_dir\n",
    "        self.transform = transform or get_transform()\n",
    "        self.img_names = self.filter_images_with_targets()\n",
    "\n",
    "    def filter_images_with_targets(self):\n",
    "        img_names = []\n",
    "        for img_name in os.listdir(self.img_dir):\n",
    "            if img_name.endswith('.jpg'):\n",
    "                ann_path = os.path.join(self.ann_dir, img_name.replace('.jpg', '.txt'))\n",
    "                if self.has_interested_objects(ann_path):\n",
    "                    img_names.append(img_name)\n",
    "        return img_names\n",
    "\n",
    "    def has_interested_objects(self, ann_path):\n",
    "        with open(ann_path) as f:\n",
    "            for line in f:\n",
    "                _, _, _, _, _, class_label, _, _ = map(int, line.split(',')[:8])\n",
    "                if class_label in [3, 4]:  # bicycle or car\n",
    "                    return True\n",
    "        return False\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.img_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.img_names[idx]\n",
    "        img_path = os.path.join(self.img_dir, img_name)\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        \n",
    "        orig_size = torch.tensor([image.width, image.height], dtype=torch.float32)\n",
    "        target_size = torch.tensor([896, 896], dtype=torch.float32)\n",
    "    \n",
    "        ann_path = os.path.join(self.ann_dir, img_name.replace('.jpg', '.txt'))\n",
    "        boxes, labels = self.parse_annotation(ann_path)\n",
    "        \n",
    "        scale_factor = target_size / orig_size\n",
    "\n",
    "        ## Resize bounding box according to image transform factor\n",
    "        boxes = boxes * scale_factor.repeat(2)\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        # target dictionary for object detection\n",
    "        target = {\"boxes\": boxes, \"labels\": labels}\n",
    "        return image, target\n",
    "\n",
    "    ## Filter the bicycle and car class from the annotation file\n",
    "    def parse_annotation(self, annotation_path):\n",
    "        boxes = []\n",
    "        labels = []\n",
    "        with open(annotation_path) as f:\n",
    "            for line in f:\n",
    "                x_min, y_min, width, height, _, class_label, _, _ = map(int, line.split(',')[:8])\n",
    "                if class_label in [3,4]:  # select bicycle and car\n",
    "                    boxes.append([x_min, y_min, x_min + width, y_min + height])\n",
    "                    labels.append(class_label - 2)\n",
    "        boxes = torch.tensor(boxes, dtype=torch.float32) if boxes else torch.zeros((0, 4), dtype=torch.float32)\n",
    "        labels = torch.tensor(labels, dtype=torch.int64) if labels else torch.zeros((0,), dtype=torch.int64)\n",
    "        return boxes, labels\n",
    "\n",
    "def collate_fn(batch):\n",
    "    images = torch.stack([item[0] for item in batch], 0)\n",
    "    targets = [item[1] for item in batch]\n",
    "    return images, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2422ace-e13d-49a4-ac98-dc831223b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a8e62ae-3f21-4787-a6d5-5176f5c74d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.models.detection.faster_rcnn import FasterRCNN_ResNet50_FPN_Weights\n",
    "import torch\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "def get_model(num_classes):\n",
    "    # load pretrained model\n",
    "    model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "    # number of features\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    # replace class number\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "def train_model(model, data_loader, device, num_epochs):\n",
    "    optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "    scheduler = StepLR(optimizer, step_size=10, gamma=0.1) \n",
    "    \n",
    "    model.to(device)\n",
    "    print('Start training')\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        print(f'Training epoch {epoch}')\n",
    "        \n",
    "        for images, targets in data_loader:\n",
    "            images = [image.to(device) for image in images]\n",
    "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            try:\n",
    "                loss_dict = model(images, targets)\n",
    "                losses = sum(loss for loss in loss_dict.values())\n",
    "                \n",
    "                losses.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                total_loss += losses.item()\n",
    "            except Exception as e:\n",
    "                print(f\"An error occurred: {e}\")\n",
    "                continue\n",
    "                \n",
    "        scheduler.step()\n",
    "        \n",
    "        print(f\"Epoch #{epoch} loss: {total_loss / len(data_loader)}\")\n",
    "        \n",
    "        if epoch % 3 == 0:\n",
    "            save_path = f'checkpoints_1/epoch_{epoch}.pth'\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\"Saved model to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b7e3b9b2-5c55-4adb-a04e-547fc66fa9be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading dataset\n",
      "1563\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "\n",
    "img_dir = 'VisDrone2019-DET-train/images'\n",
    "ann_dir='VisDrone2019-DET-train/annotations'\n",
    "\n",
    "print('Start loading dataset')\n",
    "train_dataset = VisDroneDataset(img_dir=img_dir, ann_dir=ann_dir, transform=None)\n",
    "data_loader = DataLoader(train_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "print(len(data_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "37ecd343-695a-4c51-82da-7a54deb983d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training\n",
      "Training epoch 0\n",
      "An error occurred: All bounding boxes should have positive height and width. Found invalid box [349.142822265625, 120.3809585571289, 351.4285583496094, 120.3809585571289] for target at index 3.\n",
      "Epoch #0 loss: 0.903699917541203\n",
      "Saved model to checkpoints_1/epoch_0.pth\n",
      "Training epoch 1\n",
      "An error occurred: All bounding boxes should have positive height and width. Found invalid box [349.142822265625, 120.3809585571289, 351.4285583496094, 120.3809585571289] for target at index 3.\n",
      "Epoch #1 loss: 0.765516576720062\n",
      "Training epoch 2\n",
      "An error occurred: All bounding boxes should have positive height and width. Found invalid box [349.142822265625, 120.3809585571289, 351.4285583496094, 120.3809585571289] for target at index 3.\n",
      "Epoch #2 loss: 0.7156013125435747\n",
      "Training epoch 3\n",
      "An error occurred: All bounding boxes should have positive height and width. Found invalid box [349.142822265625, 120.3809585571289, 351.4285583496094, 120.3809585571289] for target at index 3.\n",
      "Epoch #3 loss: 0.6799352727949581\n",
      "Saved model to checkpoints_1/epoch_3.pth\n",
      "Training epoch 4\n",
      "An error occurred: All bounding boxes should have positive height and width. Found invalid box [349.142822265625, 120.3809585571289, 351.4285583496094, 120.3809585571289] for target at index 3.\n",
      "Epoch #4 loss: 0.6538512128395143\n",
      "Training epoch 5\n",
      "An error occurred: All bounding boxes should have positive height and width. Found invalid box [349.142822265625, 120.3809585571289, 351.4285583496094, 120.3809585571289] for target at index 3.\n",
      "Epoch #5 loss: 0.6356515851436673\n",
      "Training epoch 6\n",
      "An error occurred: All bounding boxes should have positive height and width. Found invalid box [349.142822265625, 120.3809585571289, 351.4285583496094, 120.3809585571289] for target at index 3.\n",
      "Epoch #6 loss: 0.6183381307183724\n",
      "Saved model to checkpoints_1/epoch_6.pth\n",
      "Training epoch 7\n",
      "An error occurred: All bounding boxes should have positive height and width. Found invalid box [349.142822265625, 120.3809585571289, 351.4285583496094, 120.3809585571289] for target at index 3.\n",
      "Epoch #7 loss: 0.6035542771725493\n",
      "Training epoch 8\n",
      "An error occurred: All bounding boxes should have positive height and width. Found invalid box [349.142822265625, 120.3809585571289, 351.4285583496094, 120.3809585571289] for target at index 3.\n",
      "Epoch #8 loss: 0.5904118852452712\n",
      "Training epoch 9\n",
      "An error occurred: All bounding boxes should have positive height and width. Found invalid box [349.142822265625, 120.3809585571289, 351.4285583496094, 120.3809585571289] for target at index 3.\n",
      "Epoch #9 loss: 0.579336498256818\n",
      "Saved model to checkpoints_1/epoch_9.pth\n",
      "Training epoch 10\n",
      "An error occurred: All bounding boxes should have positive height and width. Found invalid box [349.142822265625, 120.3809585571289, 351.4285583496094, 120.3809585571289] for target at index 3.\n",
      "Epoch #10 loss: 0.5744415569695348\n",
      "Training epoch 11\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m model \u001b[38;5;241m=\u001b[39m get_model(num_classes)\n\u001b[0;32m----> 5\u001b[0m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[17], line 29\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, data_loader, device, num_epochs)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m---> 29\u001b[0m     images \u001b[38;5;241m=\u001b[39m [image\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m     30\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m     32\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "Cell \u001b[0;32mIn[17], line 29\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[0;32m---> 29\u001b[0m     images \u001b[38;5;241m=\u001b[39m [\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[1;32m     30\u001b[0m     targets \u001b[38;5;241m=\u001b[39m [{k: v\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t\u001b[38;5;241m.\u001b[39mitems()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[1;32m     32\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: an illegal memory access was encountered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 19\n",
    "num_classes = 3\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model = get_model(num_classes)\n",
    "train_model(model, data_loader, device, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5a5a21-e1e4-4ce1-8218-4f7fa6dbfe9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load fintune model for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dc21831d-a95a-4d1f-88b8-aa1d28df9991",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7758919-873e-496e-9e14-e85e43e98437",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "from PIL import Image, ImageDraw\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_transform():\n",
    "    transform = Compose([\n",
    "        Resize((896, 896)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transform\n",
    "\n",
    "def load_model_for_inference(path, device):\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=False, num_classes=3)\n",
    "    model.load_state_dict(torch.load(path, map_location=device))\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def inference(model, image, score_threshold_bi, score_threshold_car):\n",
    "    pred = {}\n",
    "    transform = get_transform()\n",
    "    img_transformed = transform(image).unsqueeze(0)\n",
    "    img_transformed = img_transformed.to(device)\n",
    "    with torch.no_grad():\n",
    "        predictions = model(img_transformed)\n",
    "    predictions = predictions[0]\n",
    "    \n",
    "    # All detections\n",
    "    all_labels = predictions['labels'].cpu().numpy()\n",
    "    all_scores = predictions['scores'].cpu().numpy()\n",
    "    all_boxes = predictions['boxes'].cpu().numpy()\n",
    "    \n",
    "    # Filter the detections with given thresholds\n",
    "    filtered_boxes = []\n",
    "    filtered_scores = []\n",
    "    filtered_labels = []\n",
    "\n",
    "    for label, score, box in zip(all_labels, all_scores, all_boxes):\n",
    "        if label == 1 and score > score_threshold_bi:\n",
    "            filtered_labels.append(label)\n",
    "            filtered_scores.append(score)\n",
    "            filtered_boxes.append(box)\n",
    "        elif label != 1 and score > score_threshold_car:\n",
    "            filtered_labels.append(label)\n",
    "            filtered_scores.append(score)\n",
    "            filtered_boxes.append(box)\n",
    "\n",
    "    pred['boxes'] = np.array(filtered_boxes)\n",
    "    pred['scores'] = np.array(filtered_scores)\n",
    "    pred['labels'] = np.array(filtered_labels)\n",
    "    # print(pred['scores'])\n",
    "    return pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4a79efd-363f-429e-b1eb-fc7f242e430e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import imageio\n",
    "import numpy as np\n",
    "import torch\n",
    "from filterpy.kalman import KalmanFilter\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0bd0c552",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kalman filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "9ab6533a-04fa-4a26-9148-95a93054ea27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_boxes(boxes, transformed_dim=(896, 896), new_dim=(1280, 720)):\n",
    "    # Resize the bouding box according the target frame size\n",
    "    scale_w, scale_h = new_dim[0] / transformed_dim[0], new_dim[1] / transformed_dim[1]\n",
    "    \n",
    "    adjusted_boxes = []\n",
    "    for box in boxes:\n",
    "        x1, y1, x2, y2 = box\n",
    "        adjusted_box = [x1 * scale_w, y1 * scale_h, x2 * scale_w, y2 * scale_h]\n",
    "        adjusted_boxes.append(adjusted_box)\n",
    "    \n",
    "    return adjusted_boxes\n",
    "\n",
    "def adjust_frame_size(frame, target_size=(1280, 720), macro_block_size=16):\n",
    "    # Adjust the frame size\n",
    "    target_width = int(np.ceil(target_size[0] / macro_block_size) * macro_block_size)\n",
    "    target_height = int(np.ceil(target_size[1] / macro_block_size) * macro_block_size)\n",
    "    \n",
    "    if isinstance(frame, Image.Image):\n",
    "        frame = np.array(frame)\n",
    "    \n",
    "    adjusted_frame = cv2.resize(frame, (target_width, target_height))\n",
    "    \n",
    "    return adjusted_frame\n",
    "\n",
    "def visualize_frame(frame, boxes, scores, labels, trackers):\n",
    "    ## Visualize trace and detection boxes\n",
    "    for trk in trackers:\n",
    "        if 'last_box' in trk:\n",
    "            box = trk['last_box']\n",
    "            x1, y1, x2, y2 = [int(coord) for coord in box]\n",
    "            color = (255, 255, 255) \n",
    "            cls = ''\n",
    "            ## Choose diffrent color of bounding box and trace for different class\n",
    "            if trk['type'] == 1:\n",
    "                cls, color = 'bicycle', (150, 123, 238)\n",
    "            elif trk['type'] == 2:\n",
    "                # cls, color = 'bicycle', (150, 123, 238)\n",
    "                cls, color = 'car', (123, 238, 176)\n",
    "            \n",
    "            # draw bounding box\n",
    "            frame = cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "            cv2.putText(frame, cls, (x1, y1-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "        \n",
    "        # Draw the trace using history points\n",
    "        if 'history' in trk:\n",
    "            for pt in trk['history'][-20:]:  # Use 80 most recent points only because change of the scene\n",
    "                cv2.circle(frame, pt, 3, color, -1) \n",
    "                \n",
    "        # Plot the predict points in red\n",
    "        pred_x, pred_y = int(trk['kf'].x[0]), int(trk['kf'].x[1])\n",
    "        cv2.circle(frame, (pred_x, pred_y), 5, (255, 0, 0), -1) \n",
    "\n",
    "    return frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "65818559",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_kalman():\n",
    "    kf = KalmanFilter(dim_x=4, dim_z=2)\n",
    "    dt = 1.0  # time gap\n",
    "\n",
    "    #  transition matrix\n",
    "    kf.F = np.array([[1, 0, dt, 0],\n",
    "                     [0, 1, 0, dt],\n",
    "                     [0, 0, 1, 0],\n",
    "                     [0, 0, 0, 1]])\n",
    "\n",
    "    #  measurement matrix\n",
    "    kf.H = np.array([[1, 0, 0, 0],\n",
    "                     [0, 1, 0, 0]])\n",
    "\n",
    "    kf.P *= 1000.  #  initial state covariance\n",
    "    kf.R = np.eye(2) * 10  # measurement noise\n",
    "    kf.Q = np.eye(4) * 0.1  # process noise\n",
    "\n",
    "    # dictionary containing the initialized Kalman filter instance\n",
    "    return {'kf': kf, 'missed_count': 0, 'history': []}\n",
    "\n",
    "def compute_iou(boxA, boxB):\n",
    "    ## Compute the intersection over union of two boxes\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
    "\n",
    "    boxAArea = (boxA[2] - boxA[0]) * (boxA[3] - boxA[1])\n",
    "    boxBArea = (boxB[2] - boxB[0]) * (boxB[3] - boxB[1])\n",
    "\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "def compute_cost_matrix(detections, trackers,adjusted_boxes_resized):\n",
    "    ## coumpute cost matrix for assign detection to trackers\n",
    "    cost_matrix = np.zeros((len(detections), len(trackers)), dtype=np.float32)\n",
    "    for d, det in enumerate(detections):\n",
    "        for t, trk in enumerate(trackers):\n",
    "            if 'last_box' in trk:\n",
    "                iou = compute_iou(adjusted_boxes_resized[d], trk['last_box'])\n",
    "                cost_matrix[d, t] = 1 - iou \n",
    "            else:\n",
    "                cost_matrix[d, t] = np.linalg.norm(np.array(det[:2]) - np.array(trk['kf'].x[:2].reshape(-1)))\n",
    "    return cost_matrix\n",
    "\n",
    "def assign_detections_to_trackers(detections, trackers,adjusted_boxes_resized):\n",
    "    ## assign detection to trackers using hungarian algorithm\n",
    "    cost_matrix = compute_cost_matrix(detections, trackers,adjusted_boxes_resized)\n",
    "    row_ind, col_ind = linear_sum_assignment(cost_matrix)\n",
    "    return row_ind, col_ind, cost_matrix\n",
    "\n",
    "def update_trackers(trackers, adjusted_boxes_resized, pred_scores, row_ind, col_ind, pred_labels):\n",
    "    matched_indices = set(row_ind)\n",
    "    matched_trackers = set(col_ind)\n",
    "    detections = [[(box[0] + box[2]) / 2, (box[1] + box[3]) / 2] for box in adjusted_boxes_resized]\n",
    "\n",
    "    # update the existing trackers\n",
    "    for d, t in zip(row_ind, col_ind):\n",
    "        trk = trackers[t]\n",
    "        det = detections[d]\n",
    "        trk['kf'].update(np.array([[det[0]], [det[1]]]))  # update karlman filter\n",
    "        trk['missed_count'] = 0  #  reset missed_count\n",
    "        trk['last_box'] = adjusted_boxes_resized[d]\n",
    "        trk['type'] = pred_labels[d]\n",
    "\n",
    "    # Evaluate whether to create a tracker for unassociated detections\n",
    "    for d in range(len(detections)):\n",
    "        if d not in matched_indices:\n",
    "            det = detections[d]\n",
    "            should_create_new_tracker = True\n",
    "            if should_create_new_tracker:\n",
    "                kf = initialize_kalman()\n",
    "                kf['kf'].update(np.array([[det[0]], [det[1]]]))\n",
    "                new_tracker = {\n",
    "                    'kf': kf['kf'],\n",
    "                    'missed_count': 0,\n",
    "                    'last_box':adjusted_boxes_resized[d],\n",
    "                    'history': [(int(det[0]), int(det[1]))],\n",
    "                    'type': pred_labels[d]  # 保type of target\n",
    "                }\n",
    "                trackers.append(new_tracker)\n",
    "\n",
    "    # Increase the missed_count for unassigned trackers\n",
    "    for t, trk in enumerate(trackers):\n",
    "        if t not in matched_trackers:\n",
    "            trk['missed_count'] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0c816a3b-925a-4abd-9c37-aa14e40df557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transform():\n",
    "    transform = Compose([\n",
    "        Resize((896, 896)),\n",
    "        ToTensor(),\n",
    "        Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return transform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5f0b3995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-object Tracking in videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4bc24ad-f1a9-4b14-a14b-77343f1039b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading video\n",
      "Object Tracking\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_412871/3158912976.py:42: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  pred_x, pred_y = int(trk['kf'].x[0]), int(trk['kf'].x[1])\n",
      "/tmp/ipykernel_412871/2822614040.py:50: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  pred_x, pred_y = int(trk['kf'].x[0]), int(trk['kf'].x[1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Object tracking complete\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import imageio\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "## read video\n",
    "print(\"Reading video\")\n",
    "video_path = 'videos/2.mp4'\n",
    "output_filename = 'videos/Cyclist and vehicle 2.mp4'\n",
    "video_reader = imageio.get_reader(video_path)  ## read video\n",
    "writer = imageio.get_writer(output_filename, fps=20)   \n",
    "\n",
    "## Load the detection model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = load_model_for_inference('checkpoints_1/epoch_9.pth',device)\n",
    "print(\"Object Tracking\")\n",
    "## Object tracking using kalman filter\n",
    "trackers = []\n",
    "frame_index=0\n",
    "for frame in video_reader:\n",
    "    original_dim = (896, 896)  #  Size of object detection image after transform\n",
    "    frame_resized = adjust_frame_size(frame, target_size=(1280, 720))   ## resize frame\n",
    "\n",
    "\n",
    "    frame_pil = Image.fromarray(frame_resized)\n",
    "    pred = inference(model, frame_pil, 0.4,0.8)  #Object detection\n",
    "    pred_boxes, pred_scores, pred_labels = pred['boxes'], pred['scores'], pred['labels']\n",
    "    \n",
    "    if len(pred_boxes) > 0:\n",
    "        adjusted_boxes_resized = adjust_boxes(pred_boxes, original_dim, (1280, 720))\n",
    "        detections = [[(box[0] + box[2]) / 2, (box[1] + box[3]) / 2] for box in adjusted_boxes_resized]\n",
    "        row_ind, col_ind, _ = assign_detections_to_trackers(detections, trackers,adjusted_boxes_resized)\n",
    "        update_trackers(trackers, adjusted_boxes_resized, pred_scores, row_ind, col_ind, pred_labels)  # 使用检测结果更新跟踪器状态\n",
    "    else:\n",
    "        writer.append_data(frame_resized)\n",
    "        continue\n",
    "\n",
    "    \n",
    "     # Predict\n",
    "    for trk in trackers:\n",
    "        trk['kf'].predict()\n",
    "        pred_x, pred_y = int(trk['kf'].x[0]), int(trk['kf'].x[1])\n",
    "        trk['history'].append((pred_x, pred_y))  # Update history\n",
    "\n",
    "    trackers = [trk for trk in trackers if trk['missed_count'] < 20]  # Clear unupdated trackers\n",
    "\n",
    "    # Visualize the object tracking and object detection results\n",
    "    vis_frame = visualize_frame(frame_resized, adjusted_boxes_resized, pred_scores, pred_labels, trackers)\n",
    "    # output_temp=f\"videos/temp/{frame_index}.jpg\"\n",
    "    # cv2.imwrite(output_temp,vis_frame)   ## write the results into a new video\n",
    "    writer.append_data(vis_frame)  \n",
    "\n",
    "    frame_index += 1\n",
    "\n",
    "    # print(frame_index )\n",
    "print(\"Object tracking complete\")\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cd8024-bc55-40b4-9d21-1526d4fb34b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
